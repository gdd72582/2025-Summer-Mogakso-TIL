# Su, H., et al. (2023). "Recent advancements in multimodal human–robot interaction" (Frontiers in Neurorobotics, 17:1084000)


***

## 1. 서론: 왜 "다중모달 인간-로봇 상호작용(Multimodal HRI)" 인가?

**인간-로봇 상호작용(HRI)** 연구는 로봇공학과 인공지능 발전의 중심축이다. 최근 로봇은 산업 현장 뿐 아니라 일상, 돌봄, 재활, 교육 등 ‘사람과 함께하는’ 복잡한 상황에 쓰이며, 이에 따라 **인간-로봇의 자연스러운 소통** 요구가 폭증하고 있다. 단일 감각(음성, 시각 등 한 가지)만으로는 한계가 분명하며, 인간처럼 여러 감각을 융합하여 더 직관적이고 복합적인 상호작용이 가능한 시스템, 즉 **다중모달 HRI**가 핵심 화두가 되었다.

Su et al.(2023)은 이런 변화의 흐름 위에서 다중모달 HRI의 이론적 배경, 주요 기술, 실제 적용사례, 그리고 최신 연구동향까지 체계적으로 정리한 **최신 포괄적 리뷰 논문**이다.

***

## 2. PRISMA 기반 체계적 문헌조사 방법론

본 논문은 **PRISMA(Preferred Reporting Items for Systematic Reviews and Meta-analysis)** 절차에 따라 체계적으로 논문을 선정하고, 2008~2022년 사이 Web of Science/Scopus/ProQuest에서 선정된 359편의 논문 중 227편의 경험적 연구논문을 분석 대상으로 선정하였다. 논의의 근거가 되는 논문 선정의 신뢰성과 최신성이 모두 확보된 연구[첨부파일].

***

## 3. 인간-로봇 상호작용에서 활용되는 주요 모달리티

### 3.1 오디오(음성) 모달리티

- **음성인식(Automatic Speech Recognition, ASR)**, **음성합성(Text-to-Speech)**, **자연어처리(NLP)** 등을 활용하여
    - 명령 전달, 대화, 피드백 등 자연언어 기반 소통 가능
    - 최근 Alexa, Google Assistant 등과 연동되는 가정용 로봇, 노인 돌봄 로봇 등 다양한 분야에서 활용
- **도전과제:** 잡음 환경, 억양/방언/고령자 발화 인식 정확도 등


### 3.2 시각(비전) 모달리티

- **컴퓨터비전(CV)** 기반 얼굴·객체·몸짓 인식
    - 얼굴·표정 인식을 통한 감정 추론, 사용자의 아이콘택·응시추적(시선추적), 제스처 인식으로 명령 인터페이스 확장
    - Visual Saliency(중요영역 인식), 3D Multi-Camera 등 고도화
- **딥러닝의 발전** 덕분에 손동작·표정·시선 등 복합적인 시각 신호 인식이 대폭 정밀해짐


### 3.3 햅틱(촉각)/운동감각/고유감각 모달리티

- **촉각:** 힘·압력·진동 등 촉각 센서 기반 상호작용(예: 재활, 수술로봇, 로봇의 터치 반응)
- **운동감각(kinesthetic):** 인간/로봇의 움직임(관절, 자세 등) 실시간 감지→협업, 보행보조 등에서 핵심
- **고유감각(proprioception):** 로봇 자신의 내부 상태, 관절 위치, 힘 등 인지→더 섬세하고 안전한 제어 가능


### 3.4 생체 신호 모달리티

- **EEG(뇌파)**, **EMG(근전도)**, **ECG(심전도)** 등 생체신호 활용
    - 사용자의 감정, 의도, 피로도 등 직접적인 신체 반응 정보의 로봇 해석 및 피드백

***

## 4. 다중모달 신호 처리·통합 기술

### 4.1 컴퓨터 비전

- 얼굴/제스처/몸짓 인식, 객체 탐지, 영상 기반 감정 추론, 시선 추적 등
- Deep learning 기반 인지 정확도 상승, 멀티카메라 시스템 도입 등으로 3D 공간이해, 자연스러운 상호작용 가능


### 4.2 자연어처리 (NLP)

- 음성→텍스트→의도/명령/질의 분석
- 대화 맥락 추적(Dialogue Management), 자연스러운 응답 생성(NLG), 다국어 번역 등
- 음성과 시각, 제스처 등 상황별 복합 명령 해석


### 4.3 제스처 인식

- 비디오, 거동 센서, 움직임 캡쳐 등 다양한 데이터 활용
- Skeleton Model, Deep Learning 등 신기술로 명확한 제스처 인식 및 구분 가능
- 동적·정적 손동작, 몸짓, 병행 의미 및 커뮤니케이션 해석


### 4.4 감정 인식

- 외적 신호(표정, 음성, 몸짓)와 내적 신호(EEG, HRV 등) 동시 활용
- 멀티모달 특성으로 자연스러운 감정이해 및 상황별 맞춤 반응 설계

***

## 5. 다중모달 피드백 기술 및 생성

### 5.1 TTS(음성합성) 및 음성 피드백

- 사람같은 자연스러운 TTS 엔진(딥러닝 기반)
- 상황/문맥에 따라 다양한 언어, 목소리, 감정 톤 조절


### 5.2 시각 피드백

- 로봇 표정, 라이트/화면, 몸 동작 통한 상태/에러/경로나 신호 피드백
- 행동/응답에 대한 피드백, 대화내용 실시간 화면 표시 등


### 5.3 동작(제스처) 생성

- 언어·의도와 동기화–로봇이 실제 손짓, 고개끄덕임, 손흔들기 등 ‘인간다운’ 제스처 생성
- 말/동작 동기화 효율적 시간 제어(ACE엔진 등 활용), co-speech gesture 생성↑


### 5.4 감정 표현

- 정적인 표정/색상·동적인 바디랭귀지/동작으로 감정표현
- Social robot, companion robot 등에 핵심


### 5.5 멀티모달 피드백

- 여러 신호를 동시에 활용(예: 안내음+화면+빛), 사회적 존재감(social presence), 정보 전달력 극대화
- 에러/상태/안내 등 중요 메시지에 복합 피드백 적용

***

## 6. 주요 응용 사례별 다중모달 HRI 발전

### 6.1 산업용 협동로봇(Industrial Cobots)

- HRC(Human-Robot Collaboration)로 작업 효율, 안전성 동시 확보
- 다양한 센서(시각, 힘, 근접, 토크 등) 조합/융합, 물리적 협업 시 비상정지 및 피드백 구조 강화
- 제스처+음성 등 복합 명령 입력, 사용자-로봇 양방향 실시간 피드백


### 6.2 웨어러블/이동 보조로봇

- **스마트 휠체어, 보행 도우미 등**: 카메라, 적외선, 음성, 터치, 생체신호 분야에서 다중 센서 조합으로 직관적 운행, 장애물 회피 등 실현
- 시각·음성·촉각 등 다양한 인터페이스로 사용자의 의도/환경 상황별 제어 최적화


### 6.3 외골격/로봇 의수족

- 근전도·뇌파 등 생체신호→사용자 의도 해석, 실시간 동작 피드백
- 멀티센서 기반 의사결정, 자연스러운 제어 및 피드백 가능

***

## 7. 최신 연구 동향: 모달리티별/인간·환경 중심 진화

### 7.1 신호 입력 부문

- **직관적 UI와 멀티모달 인터페이스**: 음성+제스처 통합, 사용성 향상
- **AVUI(Audiovisual UI)·감정 및 의도 이해**: 복합 감정 추출/분류, 실제 적용
- **비언어적 커뮤니케이션·이동훈련·활성 참여 촉진**: Nao 등 휴머노이드 다양한 센서 활용–소통 다양화


### 7.2 신호 출력/피드백 부문

- **실제 로봇에의 적용성 확대**: Co-speech gesture, 동적 머리/팔/눈동작 연계
- **로봇 감정 표현/피드백**: 감정 인식/생성 동시 연구, 사용자 감정에 신속한 반응


### 7.3 멀티모달 상호작용에서의 과제와 한계

- **센서 동기화 및 데이터 일치**: 복수 신호의 시간·공간 정렬 및 처리의 난이도
- **실시간성/적응성/경량화**: Edge/클라우드 협력·모바일 연산 최적화 계속 발전 필요
- **표준화 부족 및 상황 다양성**: 롱테일 활용 시나리오/미검증 상황의 자연스러운 처리 미흡
- **사용자 다양성 대응(고령자/장애인 등)**: 보편 접근성 고려한 HRI 설계

***

## 8. 결론 및 전망

- Su et al.는 현존하는 **다중모달 HRI 기술의 총람과 함께, 신호 입력·출력·실제 응용사례 및 최근 발전까지** 일목요연하게 정리
- **미래 방향**: 더 자연스럽고, 감정/의도 인식까지 아우르는 로봇–인간 상호작용이 핵심
- 질의응답/설명/자연스러운 협업·감정표현까지 가능해야 하며, 이 과정에서 신경망 기반의 딥러닝·강화학습·엣지컴퓨팅 등 최신 AI기술과 멀티센서 융합이 촉진될 전망
- **사회적·윤리적 이슈(프라이버시, 안전, 신뢰 등)**도 함께 연구되어야 함

***

## 9. Su et al.(2023) 논문 의미 요약(세미나용 결론)

- “모든 인간은 여러 감각을 동시에 동원해 세상을 바라본다. 미래의 로봇, HRI 역시 오디오·비전·촉각·생체신호 등 ‘다중모달’을 자연스럽게 통합해야 가장 인간다워진다.”
- Su et al.(2023)은 기술적 발전만이 아니라, **인간 중심의 상호작용 시나리오, 윤리, 실제 적용상 문제점까지** 폭넓게 바라보는 시각을 제시한다.
- 앞으로의 HRI/로봇 연구자는 ‘모달리티 융합’과 ‘자연스러운 피드백/설명력’, 그리고 사용자 경험의 개선이라는 키워드를 기억해야 한다.

***

## 참고 키워드 및 논문 인용

- Speech Recognition, Computer Vision, Gesture Recognition, Affective Computing, Multimodal Fusion, Assistive Robotics, Cobots, Kinesthetics, Proprioception, Human Intent Estimation, Social Robotics, Emotion Recognition, Co-Speech Gesture Generation, Multi-modal Feedback, Edge AI, HRC(Human-Robot Collaboration)
- 핵심참고: Su, H. et al. (2023). “Recent advancements in multimodal human–robot interaction.” Front Neurorobot. 17:1084000. https://pmc.ncbi.nlm.nih.gov/articles/PMC10210148/

---

<div style="text-align: center">⁂</div>

[^1]: https://pmc.ncbi.nlm.nih.gov/articles/PMC10210148/

