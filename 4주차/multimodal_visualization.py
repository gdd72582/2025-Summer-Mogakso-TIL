#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class MultimodalVisualization:
    """ë©€í‹°ëª¨ë‹¬ AI ì‹œê°í™” ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“Š ë©€í‹°ëª¨ë‹¬ AI ì‹œê°í™” ì‹œì‘ (Device: {self.device})")
    
    def visualize_fusion_comparison(self, early_fused, late_fused):
        """ìœµí•© ë°©ë²• ë¹„êµ ì‹œê°í™”"""
        print("\nğŸ“Š ìœµí•© ë°©ë²• ë¹„êµ ì‹œê°í™”")
        print("=" * 50)
        
        # íŠ¹ì§• ë¶„í¬ ë¹„êµ
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Early Fusion ë¶„í¬
        early_data = early_fused.detach().cpu().numpy().flatten()
        axes[0].hist(early_data, bins=30, alpha=0.7, color='blue')
        axes[0].set_title('Early Fusion íŠ¹ì§• ë¶„í¬')
        axes[0].set_xlabel('íŠ¹ì§• ê°’')
        axes[0].set_ylabel('ë¹ˆë„')
        
        # Late Fusion ë¶„í¬
        late_data = late_fused.detach().cpu().numpy().flatten()
        axes[1].hist(late_data, bins=30, alpha=0.7, color='red')
        axes[1].set_title('Late Fusion íŠ¹ì§• ë¶„í¬')
        axes[1].set_xlabel('íŠ¹ì§• ê°’')
        axes[1].set_ylabel('ë¹ˆë„')
        
        plt.tight_layout()
        plt.savefig('fusion_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ìœµí•© ë°©ë²• ë¹„êµ ì‹œê°í™” ì™„ë£Œ")
    
    def visualize_similarity_matrix(self, similarity_matrix, class_names=None):
        """ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”"""
        print("\nğŸ“Š ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”")
        print("=" * 50)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ì„ numpyë¡œ ë³€í™˜
        sim_matrix = similarity_matrix.detach().cpu().numpy()
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(8, 6))
        sns.heatmap(sim_matrix, annot=True, cmap='viridis', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í–‰ë ¬')
        plt.xlabel('í…ìŠ¤íŠ¸ í´ë˜ìŠ¤')
        plt.ylabel('ì´ë¯¸ì§€ ìƒ˜í”Œ')
        
        plt.tight_layout()
        plt.savefig('similarity_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™” ì™„ë£Œ")
    
    def visualize_attention_weights(self, attention_weights):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        print("\nğŸ“Š ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        print("=" * 50)
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ numpyë¡œ ë³€í™˜
        attn_weights = attention_weights.detach().cpu().numpy()
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜
        first_image_attention = attn_weights[0]
        
        plt.figure(figsize=(10, 6))
        plt.bar(range(len(first_image_attention)), first_image_attention)
        plt.title('ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜')
        plt.xlabel('í…ìŠ¤íŠ¸ í† í° ì¸ë±ìŠ¤')
        plt.ylabel('ì–´í…ì…˜ ê°€ì¤‘ì¹˜')
        plt.xticks(range(len(first_image_attention)))
        
        plt.tight_layout()
        plt.savefig('attention_weights.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™” ì™„ë£Œ")
    
    def visualize_embedding_space(self, image_embeddings, text_embeddings, class_names):
        """ì„ë² ë”© ê³µê°„ ì‹œê°í™” (t-SNE ì‚¬ìš©)"""
        print("\nğŸ“Š ì„ë² ë”© ê³µê°„ ì‹œê°í™”")
        print("=" * 50)
        
        try:
            from sklearn.manifold import TSNE
            
            # ì„ë² ë”© ê²°í•©
            combined_embeddings = torch.cat([image_embeddings, text_embeddings], dim=0)
            combined_embeddings = combined_embeddings.detach().cpu().numpy()
            
            # t-SNEë¡œ 2Dë¡œ ì°¨ì› ì¶•ì†Œ
            tsne = TSNE(n_components=2, random_state=42)
            embeddings_2d = tsne.fit_transform(combined_embeddings)
            
            # ì‹œê°í™”
            plt.figure(figsize=(10, 8))
            
            # ì´ë¯¸ì§€ ì„ë² ë”©
            n_images = image_embeddings.shape[0]
            plt.scatter(embeddings_2d[:n_images, 0], embeddings_2d[:n_images, 1], 
                       c='blue', marker='o', s=100, alpha=0.7, label='ì´ë¯¸ì§€')
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            plt.scatter(embeddings_2d[n_images:, 0], embeddings_2d[n_images:, 1], 
                       c='red', marker='s', s=100, alpha=0.7, label='í…ìŠ¤íŠ¸')
            
            # í´ë˜ìŠ¤ ë¼ë²¨ ì¶”ê°€
            for i, class_name in enumerate(class_names):
                plt.annotate(class_name, (embeddings_2d[i, 0], embeddings_2d[i, 1]), 
                           fontsize=8, ha='center')
            
            plt.title('t-SNE ì„ë² ë”© ê³µê°„ ì‹œê°í™”')
            plt.xlabel('t-SNE 1')
            plt.ylabel('t-SNE 2')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('embedding_space.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print("âœ… ì„ë² ë”© ê³µê°„ ì‹œê°í™” ì™„ë£Œ")
            
        except ImportError:
            print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ t-SNE ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    def analyze_performance_metrics(self, early_fused, late_fused, similarity_matrix):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„"""
        print("\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„")
        print("=" * 50)
        
        # íŠ¹ì§• ë‹¤ì–‘ì„± ë¶„ì„
        early_diversity = torch.std(early_fused).item()
        late_diversity = torch.std(late_fused).item()
        
        # ìœ ì‚¬ë„ ë¶„ì„
        sim_mean = torch.mean(similarity_matrix).item()
        sim_std = torch.std(similarity_matrix).item()
        sim_max = torch.max(similarity_matrix).item()
        sim_min = torch.min(similarity_matrix).item()
        
        print(f"íŠ¹ì§• ë‹¤ì–‘ì„± (í‘œì¤€í¸ì°¨):")
        print(f"  Early Fusion: {early_diversity:.4f}")
        print(f"  Late Fusion: {late_diversity:.4f}")
        
        print(f"\nìœ ì‚¬ë„ í†µê³„:")
        print(f"  í‰ê· : {sim_mean:.4f}")
        print(f"  í‘œì¤€í¸ì°¨: {sim_std:.4f}")
        print(f"  ìµœëŒ€ê°’: {sim_max:.4f}")
        print(f"  ìµœì†Œê°’: {sim_min:.4f}")
        
        # ë©”íŠ¸ë¦­ ì‹œê°í™”
        metrics = {
            'Early Fusion': early_diversity,
            'Late Fusion': late_diversity
        }
        
        plt.figure(figsize=(8, 6))
        plt.bar(metrics.keys(), metrics.values(), color=['blue', 'red'])
        plt.title('ìœµí•© ë°©ë²•ë³„ íŠ¹ì§• ë‹¤ì–‘ì„± ë¹„êµ')
        plt.ylabel('í‘œì¤€í¸ì°¨')
        plt.ylim(0, max(metrics.values()) * 1.1)
        
        for i, v in enumerate(metrics.values()):
            plt.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('performance_metrics.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š ë©€í‹°ëª¨ë‹¬ AI ì‹œê°í™” ë° ë¶„ì„")
    print("=" * 60)
    
    # ì‹œê°í™” ê°ì²´ ìƒì„±
    viz = MultimodalVisualization()
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size = 5
    image_dim = 512
    text_dim = 256
    embedding_dim = 128
    
    image_features = torch.randn(batch_size, image_dim).to(viz.device)
    text_features = torch.randn(batch_size, text_dim).to(viz.device)
    
    class_names = ["car", "pedestrian", "traffic_light", "stop_sign", "construction"]
    
    # ìœµí•© ê²°ê³¼ ìƒì„±
    early_fused = torch.randn(batch_size, 128).to(viz.device)
    late_fused = torch.randn(batch_size, 128).to(viz.device)
    
    # ìœ ì‚¬ë„ í–‰ë ¬ ìƒì„±
    similarity_matrix = torch.randn(batch_size, batch_size).to(viz.device)
    similarity_matrix = F.softmax(similarity_matrix, dim=-1)
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ìƒì„±
    attention_weights = torch.randn(batch_size, batch_size).to(viz.device)
    attention_weights = F.softmax(attention_weights, dim=-1)
    
    # ì„ë² ë”© ìƒì„±
    image_embeddings = torch.randn(batch_size, embedding_dim).to(viz.device)
    text_embeddings = torch.randn(batch_size, embedding_dim).to(viz.device)
    
    # 1. ìœµí•© ë°©ë²• ë¹„êµ ì‹œê°í™”
    viz.visualize_fusion_comparison(early_fused, late_fused)
    
    # 2. ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”
    viz.visualize_similarity_matrix(similarity_matrix, class_names)
    
    # 3. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
    viz.visualize_attention_weights(attention_weights)
    
    # 4. ì„ë² ë”© ê³µê°„ ì‹œê°í™”
    viz.visualize_embedding_space(image_embeddings, text_embeddings, class_names)
    
    # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
    viz.analyze_performance_metrics(early_fused, late_fused, similarity_matrix)
    
    print("\nâœ… ë©€í‹°ëª¨ë‹¬ AI ì‹œê°í™” ë° ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 