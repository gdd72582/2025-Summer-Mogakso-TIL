#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class MultimodalBasicTheory:
    """ë©€í‹°ëª¨ë‹¬ AI ê¸°ë³¸ ì´ë¡  ì‹¤ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ ë©€í‹°ëª¨ë‹¬ AI ê¸°ë³¸ ì´ë¡  ì‹¤ìŠµ ì‹œì‘ (Device: {self.device})")
    
    def demonstrate_early_fusion(self, image_features, text_features):
        """Early Fusion (ì¡°ê¸° ìœµí•©) ì‹œì—°"""
        print("\nğŸ“Š Early Fusion (ì¡°ê¸° ìœµí•©) ì‹œì—°")
        print("=" * 50)
        
        # ì›ì‹œ íŠ¹ì§•ì„ ê²°í•©
        combined_features = torch.cat([image_features, text_features], dim=-1)
        
        print(f"ì´ë¯¸ì§€ íŠ¹ì§• ì°¨ì›: {image_features.shape}")
        print(f"í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì›: {text_features.shape}")
        print(f"ê²°í•©ëœ íŠ¹ì§• ì°¨ì›: {combined_features.shape}")
        
        # ê²°í•©ëœ íŠ¹ì§•ì„ ì²˜ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬
        fusion_network = nn.Sequential(
            nn.Linear(combined_features.shape[-1], 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        ).to(self.device)
        
        fused_output = fusion_network(combined_features)
        print(f"ìœµí•© í›„ ì¶œë ¥ ì°¨ì›: {fused_output.shape}")
        
        return fused_output
    
    def demonstrate_late_fusion(self, image_features, text_features):
        """Late Fusion (í›„ê¸° ìœµí•©) ì‹œì—°"""
        print("\nğŸ“Š Late Fusion (í›„ê¸° ìœµí•©) ì‹œì—°")
        print("=" * 50)
        
        # ê° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
        image_network = nn.Sequential(
            nn.Linear(image_features.shape[-1], 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        ).to(self.device)
        
        text_network = nn.Sequential(
            nn.Linear(text_features.shape[-1], 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        ).to(self.device)
        
        processed_image = image_network(image_features)
        processed_text = text_network(text_features)
        
        print(f"ì²˜ë¦¬ëœ ì´ë¯¸ì§€ íŠ¹ì§•: {processed_image.shape}")
        print(f"ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ íŠ¹ì§•: {processed_text.shape}")
        
        # í›„ê¸° ìœµí•© (ê°€ì¤‘ í‰ê· )
        weights = torch.tensor([0.6, 0.4]).to(self.device)  # ì´ë¯¸ì§€ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        fused_output = weights[0] * processed_image + weights[1] * processed_text
        
        print(f"ìœµí•© í›„ ì¶œë ¥ ì°¨ì›: {fused_output.shape}")
        print(f"ê°€ì¤‘ì¹˜: ì´ë¯¸ì§€={weights[0]:.1f}, í…ìŠ¤íŠ¸={weights[1]:.1f}")
        
        return fused_output
    
    def demonstrate_contrastive_learning(self, image_features, text_features):
        """Contrastive Learning (ëŒ€ì¡°í•™ìŠµ) ì‹œì—°"""
        print("\nğŸ“Š Contrastive Learning (ëŒ€ì¡°í•™ìŠµ) ì‹œì—°")
        print("=" * 50)
        
        # íŠ¹ì§• ì •ê·œí™”
        image_features = F.normalize(image_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)
        
        # ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        similarity_matrix = torch.mm(image_features, text_features.T)
        
        print(f"ìœ ì‚¬ë„ í–‰ë ¬ í¬ê¸°: {similarity_matrix.shape}")
        print(f"ëŒ€ê°ì„  ìš”ì†Œ (ì •ë‹µ ìŒ): {torch.diag(similarity_matrix)}")
        
        # InfoNCE ì†ì‹¤ ê³„ì‚°
        temperature = 0.07
        logits = similarity_matrix / temperature
        
        # ì •ë‹µ ë¼ë²¨ (ëŒ€ê°ì„ )
        labels = torch.arange(logits.size(0)).to(self.device)
        
        # êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
        loss = F.cross_entropy(logits, labels)
        
        print(f"InfoNCE ì†ì‹¤: {loss.item():.4f}")
        print(f"ì˜¨ë„ íŒŒë¼ë¯¸í„°: {temperature}")
        
        return similarity_matrix, loss

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“ ë©€í‹°ëª¨ë‹¬ AI ê¸°ë³¸ ì´ë¡  ì‹¤ìŠµ")
    print("=" * 60)
    
    # ì‹¤ìŠµ ê°ì²´ ìƒì„±
    theory_practice = MultimodalBasicTheory()
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size = 4
    image_dim = 512
    text_dim = 256
    
    image_features = torch.randn(batch_size, image_dim).to(theory_practice.device)
    text_features = torch.randn(batch_size, text_dim).to(theory_practice.device)
    
    print(f"ìƒì„±ëœ ë”ë¯¸ ë°ì´í„°:")
    print(f"ì´ë¯¸ì§€ íŠ¹ì§•: {image_features.shape}")
    print(f"í…ìŠ¤íŠ¸ íŠ¹ì§•: {text_features.shape}")
    
    # 1. Early Fusion ì‹œì—°
    early_fused = theory_practice.demonstrate_early_fusion(image_features, text_features)
    
    # 2. Late Fusion ì‹œì—°
    late_fused = theory_practice.demonstrate_late_fusion(image_features, text_features)
    
    # 3. Contrastive Learning ì‹œì—°
    similarity_matrix, contrastive_loss = theory_practice.demonstrate_contrastive_learning(
        image_features, text_features
    )
    
    print("\nâœ… ë©€í‹°ëª¨ë‹¬ AI ê¸°ë³¸ ì´ë¡  ì‹¤ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 