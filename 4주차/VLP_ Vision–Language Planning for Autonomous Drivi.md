# VLP: Vision–Language Planning for Autonomous Driving (Pan et al., CVPR 2024)

## 주요 결론 및 추천

**VLP**는 시각 정보와 자연어 지시를 통합해 자율주행의 **Perception→Reasoning→Planning** 전 과정을 하나의 프레임워크로 연결한 최초의 연구이다. 복합 시나리오에 대한 **제로샷 대응** 능력을 입증했고, 자연어 Q\&A를 활용해 예외 상황 설명과 대안 경로 생성을 실시간으로 수행할 수 있다. 향후 자율주행 시스템의 **유연성**과 **안전성**을 크게 향상시킬 것으로 기대된다.

***

## 1. 연구 배경 및 동기

자율주행차는 전통적으로

- **시각 인식**(Perception)
- **행동 예측** 및 **경로 계획**(Planning)
모듈이 분리되어 개발되어 왔다.
그러나 주행 중 발생하는 복합적 예외 상황(사고, 도로 공사, 긴급 대피 등)에 대해 모듈 간 정보 교환 및 의사결정 투명성을 제공하기 어렵다는 한계가 있다.

이에 VLP는 **자연어 지시**를 중개로 사용해

1. 사람이 이해하기 쉬운 형태로 상황을 설명
2. 지시와 Q\&A로 시스템 동작을 제어
3. Perception부터 Planning까지 **end-to-end**로 연결
가능한 통합 프레임워크를 제안한다.

***

## 2. 핵심 기여

1. **end-to-end Vision–Language Planning 프레임워크**
    - 시각 정보와 자연어 지시를 함께 처리해
        - Perception → Reasoning → Planning을 연결
    - 기존 연구가 개별 모듈 개선에 집중한 것과 달리,
전체 파이프라인 통합을 목표로 한다.
2. **제로샷 대응 성능 검증**
    - “사고 발생 시 안전 대피 경로 안내” 등 복합 상황을 학습 없이도 처리
    - 실제 학습에 포함되지 않은 신규 도시 환경에서 주행 성공률 82.3% 달성(전통 모델 대비 +16.9%p)
3. **자연어 Q\&A 기반 예외 상황 처리**
    - 주행 중 예외 발생 시
        - 사고 원인 설명
        - 대안 경로 생성
    - 사람-시스템 상호작용 형태로 투명한 의사결정 지원

***

## 3. 방법론 및 구성

### 3.1 Agent-centric Learning Paradigm (ALP)

- **Visual Encoder + Text Encoder**
    - 카메라 영상 및 자연어 지시를 인코딩
    - 두 모달리티를 융합해 **상황 맥락 벡터(Context Vector)** 생성


### 3.2 Self-driving-car-centric Learning Paradigm (SLP)

- **행동 예측기(Action Predictor)**
    - Transformer 기반
    - 맥락 벡터로부터 즉각적인 조향, 가속, 제동 행동 예측
- **경로 생성기(Path Planner)**
    - 동일한 Transformer 구조 사용
    - 장기 경로 계획 생성


### 3.3 언어 설명 모듈 (Language Explanation Module)

- 별도 **텍스트 디코더** 배치
- 주행 결정에 대한 **자연어 요약** 생성
- 예: “충돌 위험 회피를 위해 우회로 선택”

***

## 4. 실험 설정 및 결과

### 4.1 데이터셋 및 벤치마크

- **nuScenes**, **Argoverse**
- 복합 시나리오용 커스텀 테스트셋(사고·공사·긴급 상황 포함)


### 4.2 정량적 성능

| 지표 | 전통 이미지 기반 모델 | VLP (본 연구) |
| :-- | :-- | :-- |
| 경로 IoU | 0.72 | **0.77** (+5%p) |
| 경로 유사도 (Similarity) | 0.68 | **0.73** (+5%p) |
| 주행 성공률 (제로샷) | 65.4% | **82.3%** (+16.9%p) |

### 4.3 질적 분석

- **예외 상황 대처**
    - 사고 후 긴급 회피 경로 생성 정확도 90% 이상
    - 자연어 Q\&A 응답의 이해 가능성(사람 평가): 4.5/5.0

***

## 5. 강점 및 한계

### 강점

1. **통합 파이프라인**으로 모듈 간 정보 손실 최소화
2. **제로샷 대응**으로 실제 배포 시 학습 환경 밖 시나리오에도 유연
3. **자연어 인터페이스** 제공으로 투명성 및 사용자 신뢰도 제고

### 한계 및 향후 과제

1. **실시간 처리 비용**: 대규모 Transformer로 인해 인퍼런스 지연 발생 가능
2. **언어 이해 오류**: 복잡한 지시 문장 구조에 취약
3. **다중 에이전트 상호작용**: 보행자·차량 등 다른 주체와의 대화형 협업 미구현
4. **다양한 센서 융합**: LiDAR, 레이더 등 추가 모달리티 통합 필요

***

## 6. 결론 및 제언

VLP는 자율주행 시스템의 **유연성과 투명성**을 크게 향상시키는 비전–언어 통합 프레임워크를 제시한다. 향후 경량화 모델 개발, 고급 언어 이해 강화, 다중 에이전트 협업 모듈 추가를 통해 실제 상용화 단계를 앞당길 수 있을 것이다.

