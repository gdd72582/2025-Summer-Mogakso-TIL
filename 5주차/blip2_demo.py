#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration

class BLIP2Demo:
    def __init__(self):
        """BLIP-2 ëª¨ë¸ ì´ˆê¸°í™”"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # BLIP-2 ëª¨ë¸ ë¡œë“œ (ê°€ë²¼ìš´ ë²„ì „ ì‚¬ìš©)
        self.processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
        self.model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b", 
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        ).to(self.device)
        
        # ììœ¨ì£¼í–‰ ê´€ë ¨ ì§ˆë¬¸ í…œí”Œë¦¿
        self.autonomous_driving_questions = [
            "What traffic signs are visible in this image?",
            "Are there any pedestrians in the scene?",
            "What is the traffic light status?",
            "Are there any obstacles on the road?",
            "What is the weather condition?",
            "Is this an intersection or a straight road?",
            "Are there any emergency vehicles?",
            "What is the road condition?",
            "Are there any construction zones?",
            "What is the speed limit indicated?",
            "Are there any lane markings visible?",
            "What type of vehicles are present?",
            "Is this a residential or highway area?",
            "Are there any crosswalks?",
            "What is the time of day?",
            "Are there any traffic cones or barriers?",
            "What is the road surface condition?",
            "Are there any parking signs?",
            "What is the traffic density?",
            "Are there any school zones?",
            "What is the road curvature?",
            "Are there any bus stops?",
            "What is the visibility condition?"
        ]
    
    def generate_caption(self, image_path):
        """ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if image_path.startswith('http'):
                image = Image.open(requests.get(image_path, stream=True).raw).convert('RGB')
            else:
                image = Image.open(image_path).convert('RGB')
            
            # ì…ë ¥ ì²˜ë¦¬
            inputs = self.processor(image, return_tensors="pt").to(self.device, torch.float16)
            
            # ìº¡ì…˜ ìƒì„±
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_length=50,
                    num_beams=5,
                    early_stopping=True
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            caption = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            
            return caption
            
        except Exception as e:
            return f"Error generating caption: {e}"
    
    def answer_question(self, image_path, question):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if image_path.startswith('http'):
                image = Image.open(requests.get(image_path, stream=True).raw).convert('RGB')
            else:
                image = Image.open(image_path).convert('RGB')
            
            # ì…ë ¥ ì²˜ë¦¬
            inputs = self.processor(image, question, return_tensors="pt").to(self.device, torch.float16)
            
            # ë‹µë³€ ìƒì„±
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_length=30,
                    num_beams=5,
                    early_stopping=True
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            answer = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            
            return answer
            
        except Exception as e:
            return f"Error generating answer: {e}"
    
    def generate_situation_report(self, image_path):
        """ììœ¨ì£¼í–‰ ìƒí™© ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ê¸°ë³¸ ìº¡ì…˜ ìƒì„±
            basic_caption = self.generate_caption(image_path)
            
            # ì£¼ìš” ì§ˆë¬¸ë“¤ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
            key_questions = [
                "What traffic signs are visible in this image?",
                "Are there any pedestrians in the scene?",
                "What is the traffic light status?",
                "Are there any obstacles on the road?"
            ]
            
            answers = {}
            for question in key_questions:
                answer = self.answer_question(image_path, question)
                answers[question] = answer
            
            # ìƒí™© ë³´ê³ ì„œ êµ¬ì„±
            report = {
                "basic_caption": basic_caption,
                "traffic_signs": answers.get("What traffic signs are visible in this image?", "None detected"),
                "pedestrians": answers.get("Are there any pedestrians in the scene?", "None detected"),
                "traffic_light": answers.get("What is the traffic light status?", "Not visible"),
                "obstacles": answers.get("Are there any obstacles on the road?", "None detected")
            }
            
            return report
            
        except Exception as e:
            return {"error": f"Error generating situation report: {e}"}
    
    def cross_validation_test(self, image_path):
        """êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸ - ì¼ê´€ì„± ì ê²€"""
        print(f"ğŸ” êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸: {image_path}")
        print("=" * 60)
        
        # ê¸°ë³¸ ìº¡ì…˜
        caption = self.generate_caption(image_path)
        print(f"ğŸ“ ê¸°ë³¸ ìº¡ì…˜: {caption}")
        print()
        
        # ê´€ë ¨ ì§ˆë¬¸ë“¤ë¡œ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        related_questions = [
            "What traffic signs are visible in this image?",
            "Are there any pedestrians in the scene?",
            "What is the traffic light status?",
            "Are there any obstacles on the road?",
            "What is the weather condition?",
            "Is this an intersection or a straight road?"
        ]
        
        print("â“ ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸:")
        for i, question in enumerate(related_questions, 1):
            answer = self.answer_question(image_path, question)
            print(f"  {i}. Q: {question}")
            print(f"     A: {answer}")
            print()
        
        # ìƒí™© ë³´ê³ ì„œ
        report = self.generate_situation_report(image_path)
        print("ğŸ“Š ìƒí™© ë³´ê³ ì„œ:")
        for key, value in report.items():
            if key != "basic_caption":
                print(f"  â€¢ {key}: {value}")
        
        print("\n" + "=" * 60)
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš— BLIP-2 ììœ¨ì£¼í–‰ ìƒí™© ë¶„ì„ ì‹œì‘")
    
    # BLIP-2 ë°ëª¨ ê°ì²´ ìƒì„±
    blip2_demo = BLIP2Demo()
    
    # ì˜ˆì‹œ ì´ë¯¸ì§€ ê²½ë¡œë“¤ (ì‹¤ì œë¡œëŠ” ììœ¨ì£¼í–‰ ê´€ë ¨ ì´ë¯¸ì§€ ì‚¬ìš©)
    sample_images = [
        "sample_intersection.jpg",
        "sample_pedestrian_crossing.jpg",
        "sample_traffic_light.jpg",
        "sample_construction_zone.jpg",
        "sample_highway.jpg"
    ]
    
    print("\n1. ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    for i, image_path in enumerate(sample_images[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ í…ŒìŠ¤íŠ¸
        print(f"\nì´ë¯¸ì§€ {i}: {image_path}")
        caption = blip2_demo.generate_caption(image_path)
        print(f"ìº¡ì…˜: {caption}")
    
    print("\n2. ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # íŠ¹ì • ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸
    test_image = sample_images[0]
    test_questions = [
        "What traffic signs are visible in this image?",
        "Are there any pedestrians in the scene?",
        "What is the traffic light status?"
    ]
    
    for question in test_questions:
        answer = blip2_demo.answer_question(test_image, question)
        print(f"Q: {question}")
        print(f"A: {answer}")
        print()
    
    print("\n3. ìƒí™© ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ìƒí™© ë³´ê³ ì„œ ìƒì„±
    report = blip2_demo.generate_situation_report(test_image)
    print("ìƒí™© ë³´ê³ ì„œ:")
    for key, value in report.items():
        print(f"  {key}: {value}")
    
    print("\n4. êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ì´ë¯¸ì§€)
    blip2_demo.cross_validation_test(sample_images[0])
    
    print("\nâœ… BLIP-2 ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 