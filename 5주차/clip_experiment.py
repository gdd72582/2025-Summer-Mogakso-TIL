#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import clip
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns

class CLIPExperiment:
    def __init__(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™”"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        
        # ììœ¨ì£¼í–‰ ê´€ë ¨ í´ë˜ìŠ¤ ì •ì˜
        self.classes = ["traffic light", "pedestrian", "car", "traffic sign", "construction"]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt_templates = {
            "basic": "a photo of a {}.",
            "context": "traffic scene with a {}.",
            "korean": "êµí†µì‹ í˜¸ë“±ì´ ìˆëŠ” ì‚¬ì§„"  # ê°„ë‹¨í•œ í•œêµ­ì–´ ì˜ˆì‹œ
        }
    
    def create_text_features(self, prompt_type="basic"):
        """í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„±"""
        if prompt_type == "korean":
            # í•œêµ­ì–´ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ì²˜ë¦¬
            text_inputs = torch.cat([clip.tokenize(self.prompt_templates["basic"].format(c)) for c in self.classes]).to(self.device)
        else:
            text_inputs = torch.cat([clip.tokenize(self.prompt_templates[prompt_type].format(c)) for c in self.classes]).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_inputs)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def predict_single_image(self, image_path, prompt_type="basic"):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì˜ˆì¸¡"""
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = Image.open(image_path).convert('RGB')
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„±
        text_features = self.create_text_features(prompt_type)
        
        # ì´ë¯¸ì§€ íŠ¹ì§• ìƒì„±
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        
        # ì˜ˆì¸¡ ê²°ê³¼
        predicted_class = self.classes[similarity.argmax().item()]
        confidence = similarity.max().item()
        
        return predicted_class, confidence, similarity.cpu().numpy()[0]
    
    def zero_shot_classification(self, image_paths, prompt_type="basic"):
        """Zero-shot ë¶„ë¥˜ ì‹¤í—˜"""
        results = []
        text_features = self.create_text_features(prompt_type)
        
        for image_path in image_paths:
            try:
                image = Image.open(image_path).convert('RGB')
                image_input = self.preprocess(image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    image_features = self.model.encode_image(image_input)
                    image_features /= image_features.norm(dim=-1, keepdim=True)
                
                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                predicted_class = self.classes[similarity.argmax().item()]
                confidence = similarity.max().item()
                
                results.append({
                    'image_path': image_path,
                    'predicted': predicted_class,
                    'confidence': confidence,
                    'similarities': similarity.cpu().numpy()[0]
                })
                
            except Exception as e:
                print(f"Error processing {image_path}: {e}")
        
        return results
    
    def image_text_search(self, image_paths, query_texts):
        """ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í—˜"""
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒì„±
        query_inputs = torch.cat([clip.tokenize(query) for query in query_texts]).to(self.device)
        with torch.no_grad():
            query_features = self.model.encode_text(query_inputs)
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        # ì´ë¯¸ì§€ íŠ¹ì§• ìƒì„±
        image_features_list = []
        valid_image_paths = []
        
        for image_path in image_paths:
            try:
                image = Image.open(image_path).convert('RGB')
                image_input = self.preprocess(image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    image_features = self.model.encode_image(image_input)
                    image_features /= image_features.norm(dim=-1, keepdim=True)
                
                image_features_list.append(image_features)
                valid_image_paths.append(image_path)
                
            except Exception as e:
                print(f"Error processing {image_path}: {e}")
        
        if not image_features_list:
            return []
        
        # ëª¨ë“  ì´ë¯¸ì§€ íŠ¹ì§•ì„ í•˜ë‚˜ë¡œ ê²°í•©
        all_image_features = torch.cat(image_features_list, dim=0)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = (100.0 * all_image_features @ query_features.T)
        
        # ê° ì¿¼ë¦¬ì— ëŒ€í•œ ìƒìœ„ ì´ë¯¸ì§€ ì°¾ê¸°
        search_results = []
        for i, query in enumerate(query_texts):
            similarities = similarity_matrix[:, i]
            top_indices = similarities.argsort(descending=True)
            
            query_results = []
            for j, idx in enumerate(top_indices[:5]):  # Top-5
                query_results.append({
                    'rank': j + 1,
                    'image_path': valid_image_paths[idx.item()],
                    'similarity': similarities[idx].item()
                })
            
            search_results.append({
                'query': query,
                'results': query_results
            })
        
        return search_results
    
    def visualize_results(self, results, title="CLIP Zero-shot Classification Results"):
        """ê²°ê³¼ ì‹œê°í™”"""
        if not results:
            print("No results to visualize")
            return
        
        # ì •í™•ë„ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        total = len(results)
        print(f"\n{title}")
        print(f"Total images processed: {total}")
        
        # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬
        class_counts = {}
        for result in results:
            pred = result['predicted']
            class_counts[pred] = class_counts.get(pred, 0) + 1
        
        print("\nPrediction distribution:")
        for class_name, count in class_counts.items():
            percentage = (count / total) * 100
            print(f"  {class_name}: {count} ({percentage:.1f}%)")
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = np.mean([r['confidence'] for r in results])
        print(f"\nAverage confidence: {avg_confidence:.3f}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš— CLIP ììœ¨ì£¼í–‰ ì‹¤í—˜ ì‹œì‘")
    
    # CLIP ì‹¤í—˜ ê°ì²´ ìƒì„±
    experiment = CLIPExperiment()
    
    # ì˜ˆì‹œ ì´ë¯¸ì§€ ê²½ë¡œ (ì‹¤ì œë¡œëŠ” ììœ¨ì£¼í–‰ ê´€ë ¨ ì´ë¯¸ì§€ ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ê°€ì •
    sample_images = [
        "sample_traffic_light.jpg",
        "sample_pedestrian.jpg", 
        "sample_car.jpg",
        "sample_sign.jpg",
        "sample_construction.jpg"
    ]
    
    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì •ì˜
    query_texts = [
        "red traffic light",
        "pedestrian crossing", 
        "construction cone"
    ]
    
    print("\n1. Zero-shot ë¶„ë¥˜ ì‹¤í—˜")
    print("=" * 50)
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ë¥˜
    basic_results = experiment.zero_shot_classification(sample_images, "basic")
    experiment.visualize_results(basic_results, "Basic Prompt Results")
    
    # ë§¥ë½ í¬í•¨ í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ë¥˜
    context_results = experiment.zero_shot_classification(sample_images, "context")
    experiment.visualize_results(context_results, "Context Prompt Results")
    
    print("\n2. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í—˜")
    print("=" * 50)
    
    # ê²€ìƒ‰ ì‹¤í—˜
    search_results = experiment.image_text_search(sample_images, query_texts)
    
    for query_result in search_results:
        print(f"\nQuery: '{query_result['query']}'")
        print("Top results:")
        for result in query_result['results']:
            print(f"  Rank {result['rank']}: {result['image_path']} (similarity: {result['similarity']:.3f})")
    
    print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 